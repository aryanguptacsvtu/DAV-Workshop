{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Tutorial: Data Science Workshop, IIT Bhilai\n",
    "\n",
    "**Presenter**: Dr. Gagan Raj Gupta, Associate Professor, IIT Bhilai  \n",
    "**Date**: July 8, 2025  \n",
    "**Venue**: IIT Bhilai, Chhattisgarh, India  \n",
    "**Course**: DS250 - Data Analytics and Visualization\n",
    "\n",
    "## Overview\n",
    "This Jupyter Notebook provides a hands-on introduction to Pandas for beginner engineering Data Science students. Designed for the Data Science Workshop at IIT Bhilai, it covers essential Pandas concepts and techniques for data manipulation and analysis. Complete the exercises to reinforce your skills. The notebook is optimized for Google Colab, requiring minimal setup.\n",
    "\n",
    "## Objectives\n",
    "- Master Pandas basics: Series vs. DataFrame.\n",
    "- Learn data I/O: CSV, Excel, JSON, SQL.\n",
    "- Apply performance tips: dtype, usecols, chunksize.\n",
    "- Handle missing data: drop vs. impute.\n",
    "- Detect and manage outliers: IQR, Z-score.\n",
    "- Perform type conversion and date handling.\n",
    "- Build data pipelines with method chaining and `.pipe()`.\n",
    "- Explore an open-source dataset (Titanic) in a hands-on section.\n",
    "\n",
    "## Prerequisites\n",
    "- **Environment**: Google Colab with Python 3.8+.\n",
    "- **Libraries**: Install required libraries in Colab:\n",
    "  ```bash\n",
    "  !pip install pandas numpy scikit-learn openpyxl seaborn\n",
    "  ```\n",
    "- **Datasets**:\n",
    "  - `iris.csv`: Modified Iris dataset with missing values and outliers (generate using provided script).\n",
    "  - `iris.db`: SQLite database with Iris data (generate using provided script).\n",
    "  - Titanic dataset: Loaded via `seaborn` for hands-on section.\n",
    "- **Setup**:\n",
    "  1. Run `create_iris_csv.py` and `create_iris_db.py` locally to generate `iris.csv` and `iris.db`, then upload to Colab.\n",
    "  2. Alternatively, generate `iris.csv` in Colab with the code below:\n",
    "  ```python\n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  from sklearn.datasets import load_iris\n",
    "  iris = load_iris()\n",
    "  df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "  df['species'] = [iris.target_names[i] for i in iris.target]\n",
    "  np.random.seed(42)\n",
    "  df.loc[np.random.choice(df.index, 5), 'sepal length (cm)'] = np.nan\n",
    "  df.loc[np.random.choice(df.index, 5), 'petal width (cm)'] = np.nan\n",
    "  df.loc[0, 'sepal width (cm)'] = 10.0\n",
    "  df.loc[1, 'petal length (cm)'] = 15.0\n",
    "  df.to_csv('iris.csv', index=False)\n",
    "  ```\n",
    "  3. The Titanic dataset is loaded directly via `seaborn.load_dataset('titanic')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries\n",
    "!pip install pandas numpy scikit-learn openpyxl seaborn\n",
    "\n",
    "# Generate iris.csv (optional, if not uploaded)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "df['species'] = [iris.target_names[i] for i in iris.target]\n",
    "np.random.seed(42)\n",
    "df.loc[np.random.choice(df.index, 5), 'sepal length (cm)'] = np.nan\n",
    "df.loc[np.random.choice(df.index, 5), 'petal width (cm)'] = np.nan\n",
    "df.loc[0, 'sepal width (cm)'] = 10.0\n",
    "df.loc[1, 'petal length (cm)'] = 15.0\n",
    "df.to_csv('iris.csv', index=False)\n",
    "print('iris.csv created:')\n",
    "print(df.head())\n",
    "\n",
    "# Verify imports\n",
    "import sqlite3\n",
    "import seaborn as sns\n",
    "print(pd.__version__, np.__version__, sns.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pandas Basics: Series vs. DataFrame\n",
    "Pandas is a powerful library for data manipulation. Its core structures are:\n",
    "- **Series**: A one-dimensional array with an index, like a column in a spreadsheet.\n",
    "- **DataFrame**: A two-dimensional table with rows and columns, like a spreadsheet.\n",
    "\n",
    "Series are used for single-column operations, while DataFrames handle tabular data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a Series\n",
    "series = pd.Series([5, 15, 25, 35], index=['p', 'q', 'r', 's'])\n",
    "print('Series:')\n",
    "print(series)\n",
    "print('Value at r:', series['r'])\n",
    "print('Add 10:', series + 10)\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet'],\n",
    "    'Price': [1200, 800, 300],\n",
    "    'Stock': [50, 100, 75]\n",
    "})\n",
    "print('\\nDataFrame:')\n",
    "print(df)\n",
    "print('Price column (Series):', df['Price'])\n",
    "print('Max Price:', df['Price'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Create a Series with values [10, 20, 30, 40] and indices ['a', 'b', 'c', 'd']. Print the Series and access the value at index 'c'.\n",
    "2. Create a DataFrame with columns 'Name' (['Alice', 'Bob', 'Charlie']), 'Age' ([25, 30, 35]), and 'City' (['Delhi', 'Mumbai', 'Bangalore']). Filter rows where Age > 28."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 1.1 and 1.2\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 1.1: Series\n",
    "# Your code here\n",
    "\n",
    "# Exercise 1.2: DataFrame\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data I/O: CSV, Excel, JSON, SQL\n",
    "Pandas supports reading and writing data in multiple formats:\n",
    "- **CSV**: Comma-separated values, lightweight.\n",
    "- **Excel**: Spreadsheet format, supports multiple sheets.\n",
    "- **JSON**: Lightweight, used for APIs.\n",
    "- **SQL**: Database integration for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Read CSV\n",
    "df_iris = pd.read_csv('iris.csv')\n",
    "print('Last 5 rows of iris.csv:')\n",
    "print(df_iris.tail())\n",
    "print('Column types:', df_iris.dtypes)\n",
    "print('Row count:', len(df_iris))\n",
    "\n",
    "# Write and read Excel\n",
    "df_sample = pd.DataFrame({'EmployeeID': [1, 2, 3], 'Name': ['Alice', 'Bob', 'Charlie'], 'Salary': [50000, 60000, 70000]})\n",
    "df_sample.to_excel('output.xlsx', index=False)\n",
    "print('\\nExcel file created:')\n",
    "print(pd.read_excel('output.xlsx'))\n",
    "\n",
    "# Write and read JSON\n",
    "df_sample.to_json('output.json', orient='records')\n",
    "print('\\nJSON file read:')\n",
    "print(pd.read_json('output.json'))\n",
    "\n",
    "# Read SQL\n",
    "conn = sqlite3.connect('iris.db')\n",
    "df_sql = pd.read_sql('SELECT * FROM iris WHERE \"sepal length (cm)\" > 5', conn)\n",
    "print('\\nSQL query result:')\n",
    "print(df_sql.head())\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Load `iris.csv` and save the first 10 rows to `iris_subset.csv`.\n",
    "2. Create a DataFrame with 3 rows and columns ['ID', 'Product', 'Price']. Save it as `products.xlsx` and `products.json`.\n",
    "3. Query `iris.db` to select rows where `petal length (cm)` > 4 and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 2.1, 2.2, and 2.3\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Exercise 2.1: CSV\n",
    "# Your code here\n",
    "\n",
    "# Exercise 2.2: Excel and JSON\n",
    "# Your code here\n",
    "\n",
    "# Exercise 2.3: SQL\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Tips: dtype, usecols, chunksize\n",
    "Optimizing Pandas performance is crucial for large datasets:\n",
    "- **dtype**: Specify column data types to reduce memory usage.\n",
    "- **usecols**: Load only necessary columns.\n",
    "- **chunksize**: Process large files in chunks to manage memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify dtypes\n",
    "dtypes = {'sepal length (cm)': 'float32', 'sepal width (cm)': 'float32', 'petal length (cm)': 'float32', 'petal width (cm)': 'float32', 'species': 'category'}\n",
    "df = pd.read_csv('iris.csv')\n",
    "df_opt = pd.read_csv('iris.csv', dtype=dtypes)\n",
    "print('Memory usage (default):', df.memory_usage().sum())\n",
    "print('Memory usage (optimized):', df_opt.memory_usage().sum())\n",
    "\n",
    "# Use usecols\n",
    "df_cols = pd.read_csv('iris.csv', usecols=['sepal length (cm)', 'species'])\n",
    "print('\\nSelected columns:')\n",
    "print(df_cols.head())\n",
    "\n",
    "# Process in chunks\n",
    "chunk_size = 50\n",
    "count = 0\n",
    "for chunk in pd.read_csv('iris.csv', chunksize=chunk_size):\n",
    "    count += len(chunk[chunk['sepal width (cm)'] > 3])\n",
    "print('\\nRows with sepal width > 3:', count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Load `iris.csv` with `float32` for numerical columns and `category` for `species`. Compare memory usage.\n",
    "2. Load only `petal length (cm)` and `petal width (cm)` from `iris.csv` and compute their mean.\n",
    "3. Process `iris.csv` in chunks of 30 rows, counting rows where `sepal length (cm)` > 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 3.1, 3.2, and 3.3\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 3.1: dtypes\n",
    "# Your code here\n",
    "\n",
    "# Exercise 3.2: usecols\n",
    "# Your code here\n",
    "\n",
    "# Exercise 3.3: chunksize\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Data: Drop vs. Impute\n",
    "Missing data can skew analysis. Pandas offers:\n",
    "- **Drop**: Remove rows or columns with missing values.\n",
    "- **Impute**: Fill missing values with statistics (mean, median) or other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load iris.csv with missing values\n",
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "# Drop missing\n",
    "print('Drop rows with any missing:')\n",
    "print(df.dropna().shape)\n",
    "print('Drop rows with missing sepal length:')\n",
    "print(df.dropna(subset=['sepal length (cm)']).shape)\n",
    "\n",
    "# Impute missing\n",
    "df_mean = df.copy()\n",
    "df_mean['sepal length (cm)'].fillna(df_mean['sepal length (cm)'].mean(), inplace=True)\n",
    "print('\\nImpute sepal length with mean:')\n",
    "print(df_mean['sepal length (cm)'].head(10))\n",
    "\n",
    "df_median = df.copy()\n",
    "df_median['petal width (cm)'].fillna(df_median['petal width (cm)'].median(), inplace=True)\n",
    "print('Impute petal width with median:')\n",
    "print(df_median['petal width (cm)'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Load `iris.csv` and drop rows with more than 1 missing value.\n",
    "2. Impute missing `sepal length (cm)` with mean and `petal width (cm)` with median. Compare their means before and after imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 4.1 and 4.2\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 4.1: Drop\n",
    "# Your code here\n",
    "\n",
    "# Exercise 4.2: Impute\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outliers: IQR, Z-score\n",
    "Outliers are extreme values that can distort analysis. Common detection methods:\n",
    "- **IQR**: Uses interquartile range (Q1 - 1.5*IQR, Q3 + 1.5*IQR).\n",
    "- **Z-score**: Measures how many standard deviations a value is from the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load iris.csv with outliers\n",
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "# IQR method\n",
    "Q1 = df['sepal width (cm)'].quantile(0.25)\n",
    "Q3 = df['sepal width (cm)'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = df[(df['sepal width (cm)'] < Q1 - 1.5 * IQR) | (df['sepal width (cm)'] > Q3 + 1.5 * IQR)]\n",
    "print('IQR outliers in sepal width:')\n",
    "print(outliers_iqr['sepal width (cm)'])\n",
    "\n",
    "# Z-score method\n",
    "z_scores = np.abs((df['sepal width (cm)'] - df['sepal width (cm)'].mean()) / df['sepal width (cm)'].std())\n",
    "outliers_z = df[z_scores > 2.5]\n",
    "print('\\nZ-score outliers in sepal width:')\n",
    "print(outliers_z['sepal width (cm)'])\n",
    "\n",
    "# Cap outliers\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "df_capped = df.copy()\n",
    "df_capped['sepal width (cm)'] = df_capped['sepal width (cm)'].clip(lower=lower_bound, upper=upper_bound)\n",
    "print('\\nCapped sepal width:')\n",
    "print(df_capped['sepal width (cm)'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Detect outliers in `petal length (cm)` using IQR and Z-score methods.\n",
    "2. Cap outliers in `petal length (cm)` at IQR bounds and print the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 5.1 and 5.2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Exercise 5.1: Detect outliers\n",
    "# Your code here\n",
    "\n",
    "# Exercise 5.2: Cap outliers\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Type Conversion & Date Handling\n",
    "Correct data types ensure efficient analysis. Pandas supports type conversion and datetime operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'order_id': ['001', '002', 'abc', '004'],\n",
    "    'amount': ['100.5', '200.7', None, '400.2'],\n",
    "    'status': ['Pending', 'Completed', 'Pending', 'Cancelled'],\n",
    "    'flag': ['1', '0', '1', '0'],\n",
    "    'order_date': ['2023-01-15', '2023-02-20', 'invalid', '2023-04-01']\n",
    "})\n",
    "\n",
    "# Convert types\n",
    "df['order_id'] = pd.to_numeric(df['order_id'], errors='coerce')\n",
    "df['amount'] = pd.to_numeric(df['amount'], errors='coerce')\n",
    "df['status'] = df['status'].astype('category')\n",
    "df['flag'] = df['flag'].astype('bool')\n",
    "df['order_date'] = pd.to_datetime(df['order_date'], errors='coerce')\n",
    "\n",
    "# Date handling\n",
    "df['year'] = df['order_date'].dt.year\n",
    "df['month'] = df['order_date'].dt.month\n",
    "df['day_of_week'] = df['order_date'].dt.day_name()\n",
    "print('Converted DataFrame:')\n",
    "print(df)\n",
    "print('Data types:', df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Load `iris.csv` and convert `species` to categorical and numerical columns to `float32`.\n",
    "2. Create a sample DataFrame with a `date` column (['2023-01-01', '2023-02-01', 'invalid']). Convert to datetime and extract year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 6.1 and 6.2\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 6.1: Type conversion\n",
    "# Your code here\n",
    "\n",
    "# Exercise 6.2: Date handling\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Pipelines: Method Chaining with .pipe()\n",
    "Pipelines streamline data processing by chaining operations. The `.pipe()` method applies custom functions to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define pipeline functions\n",
    "def impute_missing(df, column, strategy='mean'):\n",
    "    if strategy == 'mean':\n",
    "        df[column].fillna(df[column].mean(), inplace=True)\n",
    "    elif strategy == 'median':\n",
    "        df[column].fillna(df[column].median(), inplace=True)\n",
    "    return df\n",
    "\n",
    "def standardize_column(df, column):\n",
    "    df[column] = (df[column] - df[column].mean()) / df[column].std()\n",
    "    return df\n",
    "\n",
    "# Load iris.csv\n",
    "df = pd.read_csv('iris.csv')\n",
    "\n",
    "# Create pipeline\n",
    "df_clean = (df.pipe(impute_missing, 'sepal length (cm)', 'mean')\n",
    "              .pipe(standardize_column, 'sepal length (cm)'))\n",
    "print('Pipeline result:')\n",
    "print(df_clean['sepal length (cm)'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Define a function to cap outliers using IQR bounds.\n",
    "2. Create a pipeline for `iris.csv` that imputes missing `petal width (cm)` with median, caps outliers in `petal width (cm)`, and converts `species` to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 7.1 and 7.2\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 7.1: Define function\n",
    "# Your code here\n",
    "\n",
    "# Exercise 7.2: Pipeline\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hands-On: Load and Inspect Open-Source Dataset\n",
    "The Titanic dataset is a classic open-source dataset containing passenger information from the Titanic disaster. Weâ€™ll load it via `seaborn`, inspect its structure, and perform basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Load Titanic dataset\n",
    "df_titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Inspect dataset\n",
    "print('First 5 rows:')\n",
    "print(df_titanic.head())\n",
    "print('\\nInfo:')\n",
    "print(df_titanic.info())\n",
    "print('\\nMissing values:')\n",
    "print(df_titanic.isnull().sum())\n",
    "print('\\nSummary statistics:')\n",
    "print(df_titanic.describe())\n",
    "\n",
    "# Basic analysis\n",
    "print('\\nSurvival rate by class:')\n",
    "print(df_titanic.groupby('class')['survived'].mean())\n",
    "print('\\nAverage age by sex:')\n",
    "print(df_titanic.groupby('sex')['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "1. Calculate the percentage of missing values in each column of the Titanic dataset.\n",
    "2. Group by `embarked` and compute the average `fare`.\n",
    "3. Create a pipeline to impute missing `age` with median and convert `sex` to categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here for Exercise 8.1, 8.2, and 8.3\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Exercise 8.1: Missing values\n",
    "# Your code here\n",
    "\n",
    "# Exercise 8.2: Group by\n",
    "# Your code here\n",
    "\n",
    "# Exercise 8.3: Pipeline\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- **DS250 Course Page**: [https://github.com/gagan-iitb/DataAnalyticsAndVisualization](https://github.com/gagan-iitb/DataAnalyticsAndVisualization)\n",
    "- **Pandas Documentation**: [https://pandas.pydata.org/docs/](https://pandas.pydata.org/docs/)\n",
    "- **seaborn Documentation**: [https://seaborn.pydata.org/](https://seaborn.pydata.org/)\n",
    "- **Contact**: Dr. Gagan Raj Gupta, IIT Bhilai\n",
    "\n",
    "## Next Steps\n",
    "- Explore additional Pandas features: merging, reshaping, or time series analysis.\n",
    "- Analyze other open-source datasets (e.g., Kaggle datasets).\n",
    "- Create a presentation summarizing this tutorial for the workshop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}